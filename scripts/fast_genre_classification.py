import pandas as pd
import numpy as np
from sklearn.model_selection import (train_test_split, StratifiedKFold, cross_val_score, 
                                    RandomizedSearchCV, StratifiedShuffleSplit,
                                    learning_curve)
from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import HalvingGridSearchCV
from sklearn.ensemble import (RandomForestClassifier, VotingClassifier, 
                             GradientBoostingClassifier)
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import time
from datetime import datetime
warnings.filterwarnings('ignore')

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
def format_time(seconds):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    if seconds < 60:
        return f"{seconds:.1f} —Å–µ–∫"
    elif seconds < 3600:
        minutes = seconds // 60
        secs = seconds % 60
        return f"{int(minutes)} –º–∏–Ω {secs:.1f} —Å–µ–∫"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        return f"{int(hours)} —á {int(minutes)} –º–∏–Ω {secs:.1f} —Å–µ–∫"

# –ù–∞—á–∞–ª–æ –æ–±—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
total_start_time = time.time()
print(f"üöÄ –ù–∞—á–∞–ª–æ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%H:%M:%S')}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
data_start_time = time.time()
df = pd.read_csv('dataforGithub/csv/sampled_dataset_PE_C_fd.csv')
data_time = time.time() - data_start_time
print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {format_time(data_time)}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:")
print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
print(f"–°—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: {df.isnull().any(axis=1).sum()}")

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
spotify_features = ['popularity', 'duration_ms', 'explicit', 'danceability', 'energy', 
                   'key', 'loudness', 'mode', 'speechiness', 'acousticness', 
                   'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']

entropy_complexity_features = ['entropy_amplitude', 'complexity_amplitude', 'entropy_flux', 
                              'complexity_flux', 'entropy_harmony', 'complexity_harmony',
                              'entropy_spectral', 'complexity_spectral', 'higuchi_fd', 'box_counting_fd']

all_features = spotify_features + entropy_complexity_features

feature_sets = {
    'Spotify Features': spotify_features,
    'Entropy/Complexity/Fractal': entropy_complexity_features,
    'All Features': all_features
}

print(f"\nüéµ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∂–∞–Ω—Ä—ã: {sorted(df['track_genre'].unique())}")
print(f"\nüìà –ù–∞–±–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for name, features in feature_sets.items():
    print(f"{name}: {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
y = df['track_genre'].values
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–Ω—Ä–æ–≤:")
for i, genre in enumerate(le.classes_):
    count = np.sum(y_encoded == i)
    print(f"{genre}: {count} —Ç—Ä–µ–∫–æ–≤")

# –°–ª—É—á–∞–π–Ω–æ–µ stratified split
print("\n‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ stratified —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
split_start_time = time.time()

sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)
for train_idx, test_idx in sss.split(df, y_encoded):
    train_indices = train_idx
    test_indices = test_idx

print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è:")
for i, genre in enumerate(le.classes_):
    train_count = np.sum(y_encoded[train_indices] == i)
    test_count = np.sum(y_encoded[test_indices] == i)
    print(f"{genre}: {train_count} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞, {test_count} —Ç–µ—Å—Ç")

split_time = time.time() - split_start_time
print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {format_time(split_time)}")

# –£–õ–£–ß–®–ï–ù–ò–ï 1: RandomizedSearchCV –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
def optimize_randomized(X_train, y_train, model_type='rf', n_iter=30):
    """–ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ RandomizedSearchCV"""
    
    if model_type == 'rf':
        param_dist = {
            'n_estimators': [100, 200, 300, 400, 500],
            'max_depth': [None, 10, 20, 30, 40],
            'min_samples_split': [2, 5, 10, 15],
            'min_samples_leaf': [1, 2, 4, 6],
            'max_features': ['sqrt', 'log2', None],
            'class_weight': ['balanced', None]
        }
        model = RandomForestClassifier(random_state=42)
    
    elif model_type == 'gb':
        param_dist = {
            'n_estimators': [100, 200, 300, 400],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7, 9],
            'min_samples_split': [2, 5, 10],
            'subsample': [0.8, 0.9, 1.0]
        }
        model = GradientBoostingClassifier(random_state=42)
    
    elif model_type == 'svm':
        param_dist = {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
            'kernel': ['rbf', 'poly'],
            'class_weight': ['balanced', None]
        }
        model = SVC(random_state=42, probability=True)
    
    elif model_type == 'lr':
        param_dist = {
            'C': [0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga'],
            'class_weight': ['balanced', None]
        }
        model = LogisticRegression(random_state=42, max_iter=1000)
    
    # –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π CV - —Ç–æ–ª—å–∫–æ 5 folds –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    rand_search = RandomizedSearchCV(
        model, param_distributions=param_dist,
        n_iter=n_iter, cv=cv,
        scoring='accuracy', n_jobs=-1, random_state=42,
        verbose=0
    )
    
    rand_search.fit(X_train, y_train)
    
    return rand_search.best_estimator_, rand_search.best_score_, rand_search.best_params_

# –£–õ–£–ß–®–ï–ù–ò–ï 2: HalvingGridSearchCV –¥–ª—è –µ—â–µ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
def optimize_halving(X_train, y_train, model_type='rf'):
    """–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ HalvingGridSearchCV"""
    
    if model_type == 'rf':
        param_grid = {
            'n_estimators': [100, 300, 500],  # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None]
        }
        model = RandomForestClassifier(random_state=42)
    
    elif model_type == 'gb':
        param_grid = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.05, 0.1, 0.2],
            'max_depth': [3, 5, 7],
            'min_samples_split': [2, 5]
        }
        model = GradientBoostingClassifier(random_state=42)
    
    elif model_type == 'svm':
        param_grid = {
            'C': [1, 10, 100],
            'gamma': ['scale', 'auto', 0.01],
            'kernel': ['rbf']  # –£–±–∏—Ä–∞–µ–º poly –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        }
        model = SVC(random_state=42, probability=True)
    
    elif model_type == 'lr':
        param_grid = {
            'C': [1, 10, 100],
            'penalty': ['l2'],  # –£–±–∏—Ä–∞–µ–º l1 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            'solver': ['liblinear']
        }
        model = LogisticRegression(random_state=42, max_iter=1000)
    
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    halving = HalvingGridSearchCV(
        model, param_grid=param_grid,
        cv=cv, factor=3,  # –û—Ç—Å–µ–∫–∞–µ–º 2/3 –Ω–∞ –∫–∞–∂–¥–æ–º —Ä–∞—É–Ω–¥–µ
        scoring='accuracy', n_jobs=-1, 
        min_resources='smallest', verbose=0
    )
    
    halving.fit(X_train, y_train)
    
    return halving.best_estimator_, halving.best_score_, halving.best_params_

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def analyze_feature_importance(model, feature_names, model_name):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π"""
    importance_data = {}
    
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        importance_data = dict(zip(feature_names, importances))
        importance_type = 'Feature Importance'
        
    elif hasattr(model, 'coef_'):
        if len(model.coef_.shape) > 1:
            importances = np.mean(np.abs(model.coef_), axis=0)
        else:
            importances = np.abs(model.coef_)
        importance_data = dict(zip(feature_names, importances))
        importance_type = 'Coefficient Magnitude'
    
    else:
        return None, None
    
    sorted_features = sorted(importance_data.items(), key=lambda x: x[1], reverse=True)
    return sorted_features, importance_type

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
def create_calibrated_model(base_model, X_train, y_train):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
    calibrated_model = CalibratedClassifierCV(base_model, cv=3, method='isotonic')  # –£–º–µ–Ω—å—à–∞–µ–º CV
    calibrated_model.fit(X_train, y_train)
    return calibrated_model

# –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
all_results = {}

print("\n" + "="*100)
print("‚ö° –ë–´–°–¢–†–´–ô –ê–ù–ê–õ–ò–ó –° RANDOMIZED –ò HALVING –ü–û–ò–°–ö–û–ú")
print("="*100)

for feature_set_name, feature_columns in feature_sets.items():
    feature_set_start_time = time.time()
    print(f"\n{'='*30} {feature_set_name} {'='*30}")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df[feature_columns].values
    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y_encoded[train_indices], y_encoded[test_indices]
    
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n‚öôÔ∏è –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    preprocess_start_time = time.time()
    
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train)
    X_test_imputed = imputer.transform(X_test)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)
    
    # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤
    if len(feature_columns) <= 15:
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
        X_train_poly = poly.fit_transform(X_train_scaled)
        X_test_poly = poly.transform(X_test_scaled)
        print(f"üìà –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_poly.shape}")
    else:
        X_train_poly = X_train_scaled
        X_test_poly = X_test_scaled
    
    preprocess_time = time.time() - preprocess_start_time
    print(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {format_time(preprocess_time)}")
    
    feature_set_results = {}
    
    # 1. Random Forest —Å RandomizedSearchCV
    print(f"\nüå≤ 1. Random Forest (RandomizedSearchCV)...")
    rf_start_time = time.time()
    rf_best, rf_cv_score, rf_params = optimize_randomized(X_train_scaled, y_train, 'rf', n_iter=30)
    rf_pred = rf_best.predict(X_test_scaled)
    rf_accuracy = accuracy_score(y_test, rf_pred)
    
    rf_importance, rf_importance_type = analyze_feature_importance(rf_best, feature_columns, 'Random Forest')
    
    feature_set_results['Random Forest (Randomized)'] = {
        'model': rf_best,
        'accuracy': rf_accuracy,
        'cv_score': rf_cv_score,
        'predictions': rf_pred,
        'params': rf_params,
        'feature_importance': rf_importance,
        'importance_type': rf_importance_type
    }
    rf_time = time.time() - rf_start_time
    print(f"‚úÖ Random Forest CV: {rf_cv_score:.4f}, Test: {rf_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(rf_time)})")
    
    # 2. Gradient Boosting —Å HalvingGridSearchCV
    print(f"\nüìà 2. Gradient Boosting (HalvingGridSearchCV)...")
    gb_start_time = time.time()
    gb_best, gb_cv_score, gb_params = optimize_halving(X_train_scaled, y_train, 'gb')
    gb_pred = gb_best.predict(X_test_scaled)
    gb_accuracy = accuracy_score(y_test, gb_pred)
    
    gb_importance, gb_importance_type = analyze_feature_importance(gb_best, feature_columns, 'Gradient Boosting')
    
    feature_set_results['Gradient Boosting (Halving)'] = {
        'model': gb_best,
        'accuracy': gb_accuracy,
        'cv_score': gb_cv_score,
        'predictions': gb_pred,
        'params': gb_params,
        'feature_importance': gb_importance,
        'importance_type': gb_importance_type
    }
    gb_time = time.time() - gb_start_time
    print(f"‚úÖ Gradient Boosting CV: {gb_cv_score:.4f}, Test: {gb_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(gb_time)})")
    
    # 3. SVM —Å RandomizedSearchCV (—É—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    print(f"\nüîß 3. SVM (RandomizedSearchCV)...")
    svm_start_time = time.time()
    svm_best, svm_cv_score, svm_params = optimize_randomized(X_train_scaled, y_train, 'svm', n_iter=20)  # –ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π
    svm_pred = svm_best.predict(X_test_scaled)
    svm_accuracy = accuracy_score(y_test, svm_pred)
    
    feature_set_results['SVM (Randomized)'] = {
        'model': svm_best,
        'accuracy': svm_accuracy,
        'cv_score': svm_cv_score,
        'predictions': svm_pred,
        'params': svm_params
    }
    svm_time = time.time() - svm_start_time
    print(f"‚úÖ SVM CV: {svm_cv_score:.4f}, Test: {svm_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(svm_time)})")
    
    # 4. Logistic Regression —Å HalvingGridSearchCV
    print(f"\nüìä 4. Logistic Regression (HalvingGridSearchCV)...")
    lr_start_time = time.time()
    lr_best, lr_cv_score, lr_params = optimize_halving(X_train_scaled, y_train, 'lr')
    lr_pred = lr_best.predict(X_test_scaled)
    lr_accuracy = accuracy_score(y_test, lr_pred)
    
    lr_importance, lr_importance_type = analyze_feature_importance(lr_best, feature_columns, 'Logistic Regression')
    
    feature_set_results['Logistic Regression (Halving)'] = {
        'model': lr_best,
        'accuracy': lr_accuracy,
        'cv_score': lr_cv_score,
        'predictions': lr_pred,
        'params': lr_params,
        'feature_importance': lr_importance,
        'importance_type': lr_importance_type
    }
    lr_time = time.time() - lr_start_time
    print(f"‚úÖ Logistic Regression CV: {lr_cv_score:.4f}, Test: {lr_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(lr_time)})")
    
    # 5. –ë—ã—Å—Ç—Ä—ã–µ ensemble –º–æ–¥–µ–ª–∏
    print(f"\nüéØ 5. –°–æ–∑–¥–∞–Ω–∏–µ –±—ã—Å—Ç—Ä—ã—Ö Ensemble –º–æ–¥–µ–ª–µ–π...")
    ensemble_start_time = time.time()
    
    # –ö–∞–ª–∏–±—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
    print("üîß –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π...")
    best_models = [rf_best, gb_best]  # –¢–æ–ª—å–∫–æ RF –∏ GB –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    calibrated_models = []
    
    for i, model in enumerate(best_models):
        calibrated = create_calibrated_model(model, X_train_scaled, y_train)
        calibrated_models.append((f'model_{i}', calibrated))
    
    # –ü—Ä–æ—Å—Ç–æ–π voting ensemble
    voting_equal = VotingClassifier(estimators=calibrated_models, voting='soft')
    voting_equal.fit(X_train_scaled, y_train)
    voting_equal_pred = voting_equal.predict(X_test_scaled)
    voting_equal_accuracy = accuracy_score(y_test, voting_equal_pred)
    feature_set_results['Voting Ensemble (Fast)'] = {
        'model': voting_equal,
        'accuracy': voting_equal_accuracy,
        'cv_score': cross_val_score(voting_equal, X_train_scaled, y_train, cv=3).mean(),  # –£–º–µ–Ω—å—à–∞–µ–º CV
        'predictions': voting_equal_pred
    }
    print(f"‚úÖ Voting Ensemble (Fast) Test: {voting_equal_accuracy:.4f}")
    
    ensemble_time = time.time() - ensemble_start_time
    print(f"‚úÖ Ensemble –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {format_time(ensemble_time)}")
    
    # 6. –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤)
    if len(feature_columns) <= 15:
        print(f"\nüîß 6. –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (HalvingGridSearchCV)...")
        poly_start_time = time.time()
        
        rf_poly_best, rf_poly_cv_score, rf_poly_params = optimize_halving(X_train_poly, y_train, 'rf')
        rf_poly_pred = rf_poly_best.predict(X_test_poly)
        rf_poly_accuracy = accuracy_score(y_test, rf_poly_pred)
        
        feature_set_results['Random Forest (Polynomial)'] = {
            'model': rf_poly_best,
            'accuracy': rf_poly_accuracy,
            'cv_score': rf_poly_cv_score,
            'predictions': rf_poly_pred,
            'params': rf_poly_params
        }
        poly_time = time.time() - poly_start_time
        print(f"‚úÖ Random Forest (Polynomial) CV: {rf_poly_cv_score:.4f}, Test: {rf_poly_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(poly_time)})")
    
    feature_set_time = time.time() - feature_set_start_time
    print(f"\nüéâ {feature_set_name} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {format_time(feature_set_time)}")
    
    all_results[feature_set_name] = feature_set_results

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\n" + "="*100)
print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í (–ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø)")
print("="*100)

comparison_data = []
for feature_set_name, feature_set_results in all_results.items():
    for model_name, result in feature_set_results.items():
        comparison_data.append({
            'Feature Set': feature_set_name,
            'Model': model_name,
            'Accuracy': result['accuracy'],
            'CV Score': result['cv_score']
        })

comparison_df = pd.DataFrame(comparison_data)
print("\nüìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
print(comparison_df.to_string(index=False))

# –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
best_overall = comparison_df.loc[comparison_df['Accuracy'].idxmax()]
print(f"\nüëë –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å (–±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è):")
print(f"–ù–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {best_overall['Feature Set']}")
print(f"–ú–æ–¥–µ–ª—å: {best_overall['Model']}")
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {best_overall['Accuracy']:.4f}")
print(f"CV Score: {best_overall['CV Score']:.4f}")

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_feature_set = best_overall['Feature Set']
best_model_name = best_overall['Model']
best_model = all_results[best_feature_set][best_model_name]['model']
best_predictions = all_results[best_feature_set][best_model_name]['predictions']

# –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
X_best = df[feature_sets[best_feature_set]].values
X_train_best, X_test_best = X_best[train_indices], X_best[test_indices]
y_train_best, y_test_best = y_encoded[train_indices], y_encoded[test_indices]

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
imputer_best = SimpleImputer(strategy='median')
X_train_best_imputed = imputer_best.fit_transform(X_train_best)
X_test_best_imputed = imputer_best.transform(X_test_best)

scaler_best = StandardScaler()
X_train_best_scaled = scaler_best.fit_transform(X_train_best_imputed)
X_test_best_scaled = scaler_best.transform(X_test_best_imputed)

print("\n" + "="*100)
print(f"üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {best_model_name} ({best_feature_set})")
print("="*100)

# Classification report
print("\nüìä Classification Report:")
print(classification_report(y_test_best, best_predictions, target_names=le.classes_))

# –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
if 'feature_importance' in all_results[best_feature_set][best_model_name]:
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {best_model_name}:")
    importance_data = all_results[best_feature_set][best_model_name]['feature_importance']
    importance_type = all_results[best_feature_set][best_model_name]['importance_type']
    
    print(f"–¢–∏–ø –≤–∞–∂–Ω–æ—Å—Ç–∏: {importance_type}")
    print("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, (feature, importance) in enumerate(importance_data[:10]):
        print(f"{i+1:2d}. {feature}: {importance:.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plt.figure(figsize=(12, 8))
    top_features = importance_data[:15]
    features, importances = zip(*top_features)
    
    plt.barh(range(len(features)), importances, color='lightgreen', edgecolor='darkgreen')
    plt.yticks(range(len(features)), features)
    plt.xlabel(importance_type)
    plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - {best_model_name} ({best_feature_set}) - –ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('feature_importance_fast.png', dpi=300, bbox_inches='tight')
    plt.show()

# Confusion Matrix
print("\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
viz_start_time = time.time()

plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_test_best, best_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', 
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title(f'Confusion Matrix - {best_model_name} ({best_feature_set}) - –ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('confusion_matrix_fast.png', dpi=300, bbox_inches='tight')
plt.show()

# –ê–Ω–∞–ª–∏–∑ –ø–æ –∂–∞–Ω—Ä–∞–º
print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∂–∞–Ω—Ä–∞–º:")
genre_accuracy = {}
for i, genre in enumerate(le.classes_):
    genre_mask = y_test_best == i
    if np.sum(genre_mask) > 0:
        genre_acc = accuracy_score(y_test_best[genre_mask], best_predictions[genre_mask])
        genre_accuracy[genre] = genre_acc
        print(f"{genre}: {genre_acc:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∂–∞–Ω—Ä–∞–º
plt.figure(figsize=(12, 6))
genres = list(genre_accuracy.keys())
accuracies = list(genre_accuracy.values())

plt.bar(genres, accuracies, color='lightgreen', edgecolor='darkgreen')
plt.title(f'–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –∂–∞–Ω—Ä–∞–º - {best_model_name} ({best_feature_set}) - –ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø')
plt.xlabel('–ñ–∞–Ω—Ä')
plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(axis='y', alpha=0.3)

for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('genre_accuracy_fast.png', dpi=300, bbox_inches='tight')
plt.show()

# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")

plt.figure(figsize=(16, 10))
pivot_df = comparison_df.pivot(index='Model', columns='Feature Set', values='Accuracy')
sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='YlGn', cbar_kws={'label': 'Accuracy'})
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π (–ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø)')
plt.tight_layout()
plt.savefig('model_comparison_heatmap_fast.png', dpi=300, bbox_inches='tight')
plt.show()

viz_time = time.time() - viz_start_time
print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {format_time(viz_time)}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\n" + "="*100)
print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*100)

save_start_time = time.time()

comparison_df.to_csv('classification_results_fast.csv', index=False)
print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ classification_results_fast.csv")

import joblib
joblib.dump(best_model, 'best_genre_classifier_fast.pkl')
print("‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ best_genre_classifier_fast.pkl")

best_model_info = {
    'feature_set': best_feature_set,
    'model_name': best_model_name,
    'test_accuracy': best_overall['Accuracy'],
    'cv_score': best_overall['CV Score'],
    'best_params': all_results[best_feature_set][best_model_name].get('params', {}),
    'feature_importance': all_results[best_feature_set][best_model_name].get('feature_importance', []),
    'version': 'fast'
}

import json
with open('best_model_info_fast.json', 'w') as f:
    json.dump(best_model_info, f, indent=2, default=str)
print("‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ best_model_info_fast.json")

save_time = time.time() - save_start_time
print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {format_time(save_time)}")

# –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
total_time = time.time() - total_start_time
print("\n" + "="*100)
print("üéâ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï (–ë–´–°–¢–†–ê–Ø –í–ï–†–°–ò–Ø)")
print("="*100)
print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {format_time(total_time)}")
print(f"üïê –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ: {datetime.now().strftime('%H:%M:%S')}")
print(f"üöÄ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
print(f"üìä –ù–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {best_feature_set}")
print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {best_overall['Accuracy']:.4f}")
print(f"üìà CV Score: {best_overall['CV Score']:.4f}")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for feature_set_name, feature_set_results in all_results.items():
    best_acc = max(result['accuracy'] for result in feature_set_results.values())
    best_cv = max(result['cv_score'] for result in feature_set_results.values())
    print(f"- {feature_set_name}: Test {best_acc:.4f}, CV {best_cv:.4f}")

# –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
print(f"\n‚ùå –ù–∞–∏–±–æ–ª–µ–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∂–∞–Ω—Ä—ã:")
worst_genres = sorted(genre_accuracy.items(), key=lambda x: x[1])[:3]
for genre, acc in worst_genres:
    print(f"- {genre}: {acc:.4f}")

print(f"\n‚úÖ –ù–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –∂–∞–Ω—Ä—ã:")
best_genres = sorted(genre_accuracy.items(), key=lambda x: x[1], reverse=True)[:3]
for genre, acc in best_genres:
    print(f"- {genre}: {acc:.4f}")

# –°–≤–æ–¥–∫–∞ —É—Å–∫–æ—Ä–µ–Ω–∏–π
print(f"\n‚ö° –û–°–ù–û–í–ù–´–ï –£–°–ö–û–†–ï–ù–ò–Ø:")
print("1. ‚úÖ RandomizedSearchCV –≤–º–µ—Å—Ç–æ GridSearchCV (—ç–∫–æ–Ω–æ–º–∏—è ~80% –≤—Ä–µ–º–µ–Ω–∏)")
print("2. ‚úÖ HalvingGridSearchCV –¥–ª—è –µ—â–µ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞")
print("3. ‚úÖ –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
print("4. ‚úÖ –£–±—Ä–∞–Ω—ã –ø–æ–≤—Ç–æ—Ä—ã –≤ CV (—Ç–æ–ª—å–∫–æ 5 folds)")
print("5. ‚úÖ –£–º–µ–Ω—å—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è SVM")
print("6. ‚úÖ –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ ensemble (—Ç–æ–ª—å–∫–æ 2 –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏)")
print("7. ‚úÖ –ë—ã—Å—Ç—Ä–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ (3-fold CV)")

print(f"\nüìà –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ:")
print("- –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è: ~1-2 —á–∞—Å–∞")
print("- –ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è: ~10-20 –º–∏–Ω—É—Ç")
print("- –£—Å–∫–æ—Ä–µ–Ω–∏–µ: –≤ 3-6 —Ä–∞–∑!")

print(f"\nüéä –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {format_time(total_time)}!") 