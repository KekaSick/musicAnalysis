import pandas as pd
import numpy as np
from sklearn.model_selection import (train_test_split, StratifiedKFold, cross_val_score, 
                                    GridSearchCV, RepeatedStratifiedKFold, StratifiedShuffleSplit,
                                    learning_curve, validation_curve)
from sklearn.ensemble import (RandomForestClassifier, VotingClassifier, 
                             GradientBoostingClassifier, ExtraTreesClassifier)
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.calibration import CalibratedClassifierCV
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import time
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
def format_time(seconds):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥"""
    if seconds < 60:
        return f"{seconds:.1f} —Å–µ–∫"
    elif seconds < 3600:
        minutes = seconds // 60
        secs = seconds % 60
        return f"{int(minutes)} –º–∏–Ω {secs:.1f} —Å–µ–∫"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        return f"{int(hours)} —á {int(minutes)} –º–∏–Ω {secs:.1f} —Å–µ–∫"

# –ù–∞—á–∞–ª–æ –æ–±—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
total_start_time = time.time()
print(f"üöÄ –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%H:%M:%S')}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("\nüìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
data_start_time = time.time()
df = pd.read_csv('dataforGithub/csv/sampled_dataset_PE_C_fd.csv')
data_time = time.time() - data_start_time
print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {format_time(data_time)}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:")
print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
print(f"–°—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: {df.isnull().any(axis=1).sum()}")
print(f"–ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:")
for col in df.columns:
    null_count = df[col].isnull().sum()
    if null_count > 0:
        print(f"  {col}: {null_count} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
spotify_features = ['popularity', 'duration_ms', 'explicit', 'danceability', 'energy', 
                   'key', 'loudness', 'mode', 'speechiness', 'acousticness', 
                   'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']

entropy_complexity_features = ['entropy_amplitude', 'complexity_amplitude', 'entropy_flux', 
                              'complexity_flux', 'entropy_harmony', 'complexity_harmony',
                              'entropy_spectral', 'complexity_spectral', 'higuchi_fd', 'box_counting_fd']

all_features = spotify_features + entropy_complexity_features

feature_sets = {
    'Spotify Features': spotify_features,
    'Entropy/Complexity/Fractal': entropy_complexity_features,
    'All Features': all_features
}

print(f"\nüéµ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∂–∞–Ω—Ä—ã: {sorted(df['track_genre'].unique())}")
print(f"\nüìà –ù–∞–±–æ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for name, features in feature_sets.items():
    print(f"{name}: {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
y = df['track_genre'].values

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∂–∞–Ω—Ä–æ–≤:")
for i, genre in enumerate(le.classes_):
    count = np.sum(y_encoded == i)
    print(f"{genre}: {count} —Ç—Ä–µ–∫–æ–≤")

# –£–õ–£–ß–®–ï–ù–ò–ï 1: –°–ª—É—á–∞–π–Ω–æ–µ stratified split –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ
print("\n‚úÇÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ stratified —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
split_start_time = time.time()

# –ò—Å–ø–æ–ª—å–∑—É–µ–º StratifiedShuffleSplit –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ, –Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)
for train_idx, test_idx in sss.split(df, y_encoded):
    train_indices = train_idx
    test_indices = test_idx

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è:")
for i, genre in enumerate(le.classes_):
    train_count = np.sum(y_encoded[train_indices] == i)
    test_count = np.sum(y_encoded[test_indices] == i)
    print(f"{genre}: {train_count} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞, {test_count} —Ç–µ—Å—Ç")

split_time = time.time() - split_start_time
print(f"‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {format_time(split_time)}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
def optimize_hyperparameters(X_train, y_train, model_type='rf'):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π"""
    
    if model_type == 'rf':
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None],
            'class_weight': ['balanced', None]
        }
        model = RandomForestClassifier(random_state=42)
    
    elif model_type == 'gb':
        param_grid = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7],
            'min_samples_split': [2, 5],
            'subsample': [0.8, 0.9, 1.0]
        }
        model = GradientBoostingClassifier(random_state=42)
    
    elif model_type == 'svm':
        param_grid = {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
            'kernel': ['rbf', 'poly'],
            'class_weight': ['balanced', None]
        }
        model = SVC(random_state=42, probability=True)
    
    elif model_type == 'lr':
        param_grid = {
            'C': [0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga'],
            'class_weight': ['balanced', None]
        }
        model = LogisticRegression(random_state=42, max_iter=1000)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º RepeatedStratifiedKFold –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)
    
    grid_search = GridSearchCV(
        model, param_grid, cv=cv, scoring='accuracy', 
        n_jobs=-1, verbose=0, return_train_score=True
    )
    
    grid_search.fit(X_train, y_train)
    
    return grid_search.best_estimator_, grid_search.best_score_, grid_search.best_params_

# –£–õ–£–ß–®–ï–ù–ò–ï 2: –§—É–Ω–∫—Ü–∏—è –¥–ª—è nested cross-validation
def nested_cross_validation(X, y, model, param_grid, outer_cv=5, inner_cv=3):
    """Nested cross-validation –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    outer_scores = []
    
    # –í–Ω–µ—à–Ω–∏–π CV
    outer_cv_splitter = StratifiedKFold(n_splits=outer_cv, shuffle=True, random_state=42)
    
    for train_idx, test_idx in outer_cv_splitter.split(X, y):
        X_train_outer, X_test_outer = X[train_idx], X[test_idx]
        y_train_outer, y_test_outer = y[train_idx], y[test_idx]
        
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π CV –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        inner_cv_splitter = RepeatedStratifiedKFold(n_splits=5, n_repeats=inner_cv, random_state=42)
        grid_search = GridSearchCV(model, param_grid, cv=inner_cv_splitter, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train_outer, y_train_outer)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–Ω–µ—à–Ω–µ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
        best_model = grid_search.best_estimator_
        score = best_model.score(X_test_outer, y_test_outer)
        outer_scores.append(score)
    
    return np.mean(outer_scores), np.std(outer_scores)

# –£–õ–£–ß–®–ï–ù–ò–ï 3: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def analyze_feature_importance(model, feature_names, model_name):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π"""
    importance_data = {}
    
    if hasattr(model, 'feature_importances_'):
        # –î–ª—è tree-based –º–æ–¥–µ–ª–µ–π
        importances = model.feature_importances_
        importance_data = dict(zip(feature_names, importances))
        importance_type = 'Feature Importance'
        
    elif hasattr(model, 'coef_'):
        # –î–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        if len(model.coef_.shape) > 1:
            # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            importances = np.mean(np.abs(model.coef_), axis=0)
        else:
            importances = np.abs(model.coef_)
        importance_data = dict(zip(feature_names, importances))
        importance_type = 'Coefficient Magnitude'
    
    else:
        return None, None
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
    sorted_features = sorted(importance_data.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_features, importance_type

# –£–õ–£–ß–®–ï–ù–ò–ï 4: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ learning curves
def plot_learning_curves(X_train, y_train, model, model_name):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ learning curves –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_sizes, train_scores, val_scores = learning_curve(
        model, X_train, y_train, train_sizes=train_sizes, 
        cv=5, n_jobs=-1, scoring='accuracy'
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')
    plt.plot(train_sizes, val_mean, label='Cross-validation score', color='red', marker='s')
    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.15, color='red')
    
    plt.xlabel('Training Examples')
    plt.ylabel('Score')
    plt.title(f'Learning Curves - {model_name}')
    plt.legend(loc='best')
    plt.grid(True)
    plt.tight_layout()
    
    return plt.gcf()

# –£–õ–£–ß–®–ï–ù–ò–ï 5: –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
def create_calibrated_model(base_model, X_train, y_train):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
    calibrated_model = CalibratedClassifierCV(base_model, cv=5, method='isotonic')
    calibrated_model.fit(X_train, y_train)
    return calibrated_model

# –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
all_results = {}

print("\n" + "="*100)
print("üöÄ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –° –£–õ–£–ß–®–ï–ù–ò–Ø–ú–ò")
print("="*100)

for feature_set_name, feature_columns in feature_sets.items():
    feature_set_start_time = time.time()
    print(f"\n{'='*30} {feature_set_name} {'='*30}")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X = df[feature_columns].values
    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y_encoded[train_indices], y_encoded[test_indices]
    
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–µ–∫—É—â–µ–º –Ω–∞–±–æ—Ä–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    train_nulls = pd.DataFrame(X_train, columns=feature_columns).isnull().sum().sum()
    test_nulls = pd.DataFrame(X_test, columns=feature_columns).isnull().sum().sum()
    print(f"üîç –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ: {train_nulls}")
    print(f"üîç –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_nulls}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n‚öôÔ∏è –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    preprocess_start_time = time.time()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    imputer = SimpleImputer(strategy='median')
    X_train_imputed = imputer.fit_transform(X_train)
    X_test_imputed = imputer.transform(X_test)
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤)
    if len(feature_columns) <= 15:
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
        X_train_poly = poly.fit_transform(X_train_scaled)
        X_test_poly = poly.transform(X_test_scaled)
        print(f"üìà –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train_poly.shape}")
    else:
        X_train_poly = X_train_scaled
        X_test_poly = X_test_scaled
    
    preprocess_time = time.time() - preprocess_start_time
    print(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {format_time(preprocess_time)}")
    
    feature_set_results = {}
    
    # 1. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Random Forest
    print(f"\nüå≤ 1. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Random Forest...")
    rf_start_time = time.time()
    rf_best, rf_cv_score, rf_params = optimize_hyperparameters(X_train_scaled, y_train, 'rf')
    rf_pred = rf_best.predict(X_test_scaled)
    rf_accuracy = accuracy_score(y_test, rf_pred)
    
    # –£–õ–£–ß–®–ï–ù–ò–ï: –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è RF
    rf_importance, rf_importance_type = analyze_feature_importance(rf_best, feature_columns, 'Random Forest')
    
    # –£–õ–£–ß–®–ï–ù–ò–ï: Learning curves –¥–ª—è RF
    print("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ learning curves –¥–ª—è Random Forest...")
    rf_lc_fig = plot_learning_curves(X_train_scaled, y_train, rf_best, 'Random Forest')
    rf_lc_fig.savefig(f'learning_curves_rf_{feature_set_name.replace(" ", "_").lower()}.png', dpi=300, bbox_inches='tight')
    plt.close(rf_lc_fig)
    
    feature_set_results['Random Forest (Optimized)'] = {
        'model': rf_best,
        'accuracy': rf_accuracy,
        'cv_score': rf_cv_score,
        'predictions': rf_pred,
        'params': rf_params,
        'feature_importance': rf_importance,
        'importance_type': rf_importance_type
    }
    rf_time = time.time() - rf_start_time
    print(f"‚úÖ Random Forest CV: {rf_cv_score:.4f}, Test: {rf_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(rf_time)})")
    
    # 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Gradient Boosting
    print(f"\nüìà 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Gradient Boosting...")
    gb_start_time = time.time()
    gb_best, gb_cv_score, gb_params = optimize_hyperparameters(X_train_scaled, y_train, 'gb')
    gb_pred = gb_best.predict(X_test_scaled)
    gb_accuracy = accuracy_score(y_test, gb_pred)
    
    # –£–õ–£–ß–®–ï–ù–ò–ï: –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è GB
    gb_importance, gb_importance_type = analyze_feature_importance(gb_best, feature_columns, 'Gradient Boosting')
    
    feature_set_results['Gradient Boosting (Optimized)'] = {
        'model': gb_best,
        'accuracy': gb_accuracy,
        'cv_score': gb_cv_score,
        'predictions': gb_pred,
        'params': gb_params,
        'feature_importance': gb_importance,
        'importance_type': gb_importance_type
    }
    gb_time = time.time() - gb_start_time
    print(f"‚úÖ Gradient Boosting CV: {gb_cv_score:.4f}, Test: {gb_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(gb_time)})")
    
    # 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVM
    print(f"\nüîß 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è SVM...")
    svm_start_time = time.time()
    svm_best, svm_cv_score, svm_params = optimize_hyperparameters(X_train_scaled, y_train, 'svm')
    svm_pred = svm_best.predict(X_test_scaled)
    svm_accuracy = accuracy_score(y_test, svm_pred)
    
    feature_set_results['SVM (Optimized)'] = {
        'model': svm_best,
        'accuracy': svm_accuracy,
        'cv_score': svm_cv_score,
        'predictions': svm_pred,
        'params': svm_params
    }
    svm_time = time.time() - svm_start_time
    print(f"‚úÖ SVM CV: {svm_cv_score:.4f}, Test: {svm_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(svm_time)})")
    
    # 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Logistic Regression
    print(f"\nüìä 4. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Logistic Regression...")
    lr_start_time = time.time()
    lr_best, lr_cv_score, lr_params = optimize_hyperparameters(X_train_scaled, y_train, 'lr')
    lr_pred = lr_best.predict(X_test_scaled)
    lr_accuracy = accuracy_score(y_test, lr_pred)
    
    # –£–õ–£–ß–®–ï–ù–ò–ï: –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LR
    lr_importance, lr_importance_type = analyze_feature_importance(lr_best, feature_columns, 'Logistic Regression')
    
    feature_set_results['Logistic Regression (Optimized)'] = {
        'model': lr_best,
        'accuracy': lr_accuracy,
        'cv_score': lr_cv_score,
        'predictions': lr_pred,
        'params': lr_params,
        'feature_importance': lr_importance,
        'importance_type': lr_importance_type
    }
    lr_time = time.time() - lr_start_time
    print(f"‚úÖ Logistic Regression CV: {lr_cv_score:.4f}, Test: {lr_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(lr_time)})")
    
    # 5. –£–õ–£–ß–®–ï–ù–ò–ï: –ö–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ ensemble –º–æ–¥–µ–ª–∏
    print(f"\nüéØ 5. –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö Ensemble –º–æ–¥–µ–ª–µ–π...")
    ensemble_start_time = time.time()
    
    # –ö–∞–ª–∏–±—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    print("üîß –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    rf_calibrated = create_calibrated_model(rf_best, X_train_scaled, y_train)
    gb_calibrated = create_calibrated_model(gb_best, X_train_scaled, y_train)
    svm_calibrated = create_calibrated_model(svm_best, X_train_scaled, y_train)
    lr_calibrated = create_calibrated_model(lr_best, X_train_scaled, y_train)
    
    # Ensemble —Å –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
    calibrated_models = [
        ('rf_cal', rf_calibrated),
        ('gb_cal', gb_calibrated),
        ('svm_cal', svm_calibrated),
        ('lr_cal', lr_calibrated)
    ]
    
    # Voting ensemble —Å —Ä–∞–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
    voting_equal = VotingClassifier(estimators=calibrated_models, voting='soft')
    voting_equal.fit(X_train_scaled, y_train)
    voting_equal_pred = voting_equal.predict(X_test_scaled)
    voting_equal_accuracy = accuracy_score(y_test, voting_equal_pred)
    feature_set_results['Voting Ensemble (Calibrated, Equal)'] = {
        'model': voting_equal,
        'accuracy': voting_equal_accuracy,
        'cv_score': cross_val_score(voting_equal, X_train_scaled, y_train, cv=5).mean(),
        'predictions': voting_equal_pred
    }
    print(f"‚úÖ Voting Ensemble (Calibrated, Equal) Test: {voting_equal_accuracy:.4f}")
    
    # Voting ensemble —Å –≤–µ—Å–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ CV scores
    weights = [rf_cv_score, gb_cv_score, svm_cv_score, lr_cv_score]
    voting_weighted = VotingClassifier(estimators=calibrated_models, voting='soft', weights=weights)
    voting_weighted.fit(X_train_scaled, y_train)
    voting_weighted_pred = voting_weighted.predict(X_test_scaled)
    voting_weighted_accuracy = accuracy_score(y_test, voting_weighted_pred)
    feature_set_results['Voting Ensemble (Calibrated, Weighted)'] = {
        'model': voting_weighted,
        'accuracy': voting_weighted_accuracy,
        'cv_score': cross_val_score(voting_weighted, X_train_scaled, y_train, cv=5).mean(),
        'predictions': voting_weighted_pred
    }
    print(f"‚úÖ Voting Ensemble (Calibrated, Weighted) Test: {voting_weighted_accuracy:.4f}")
    
    ensemble_time = time.time() - ensemble_start_time
    print(f"‚úÖ Ensemble –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {format_time(ensemble_time)}")
    
    # 6. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    if len(feature_columns) <= 15:
        print(f"\nüîß 6. –ú–æ–¥–µ–ª–∏ —Å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏...")
        poly_start_time = time.time()
        
        # Random Forest —Å –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        rf_poly_best, rf_poly_cv_score, rf_poly_params = optimize_hyperparameters(X_train_poly, y_train, 'rf')
        rf_poly_pred = rf_poly_best.predict(X_test_poly)
        rf_poly_accuracy = accuracy_score(y_test, rf_poly_pred)
        
        # –£–õ–£–ß–®–ï–ù–ò–ï: Learning curves –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        print("üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ learning curves –¥–ª—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        poly_feature_names = [f"poly_{i}" for i in range(X_train_poly.shape[1])]
        rf_poly_lc_fig = plot_learning_curves(X_train_poly, y_train, rf_poly_best, 'Random Forest (Polynomial)')
        rf_poly_lc_fig.savefig(f'learning_curves_rf_poly_{feature_set_name.replace(" ", "_").lower()}.png', dpi=300, bbox_inches='tight')
        plt.close(rf_poly_lc_fig)
        
        feature_set_results['Random Forest (Polynomial)'] = {
            'model': rf_poly_best,
            'accuracy': rf_poly_accuracy,
            'cv_score': rf_poly_cv_score,
            'predictions': rf_poly_pred,
            'params': rf_poly_params
        }
        poly_time = time.time() - poly_start_time
        print(f"‚úÖ Random Forest (Polynomial) CV: {rf_poly_cv_score:.4f}, Test: {rf_poly_accuracy:.4f} (–≤—Ä–µ–º—è: {format_time(poly_time)})")
    
    feature_set_time = time.time() - feature_set_start_time
    print(f"\nüéâ {feature_set_name} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {format_time(feature_set_time)}")
    
    all_results[feature_set_name] = feature_set_results

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\n" + "="*100)
print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*100)

# –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
comparison_data = []
for feature_set_name, feature_set_results in all_results.items():
    for model_name, result in feature_set_results.items():
        comparison_data.append({
            'Feature Set': feature_set_name,
            'Model': model_name,
            'Accuracy': result['accuracy'],
            'CV Score': result['cv_score']
        })

comparison_df = pd.DataFrame(comparison_data)
print("\nüìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
print(comparison_df.to_string(index=False))

# –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print("\nüèÜ –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for feature_set_name, feature_set_results in all_results.items():
    best_model_name = max(feature_set_results.keys(), 
                         key=lambda x: feature_set_results[x]['accuracy'])
    best_accuracy = feature_set_results[best_model_name]['accuracy']
    best_cv = feature_set_results[best_model_name]['cv_score']
    print(f"{feature_set_name}: {best_model_name} - Test: {best_accuracy:.4f}, CV: {best_cv:.4f}")

# –ù–∞—Ö–æ–¥–∏–º –∞–±—Å–æ–ª—é—Ç–Ω–æ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
best_overall = comparison_df.loc[comparison_df['Accuracy'].idxmax()]
print(f"\nüëë –ê–±—Å–æ–ª—é—Ç–Ω–æ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å:")
print(f"–ù–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {best_overall['Feature Set']}")
print(f"–ú–æ–¥–µ–ª—å: {best_overall['Model']}")
print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {best_overall['Accuracy']:.4f}")
print(f"CV Score: {best_overall['CV Score']:.4f}")

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_feature_set = best_overall['Feature Set']
best_model_name = best_overall['Model']
best_model = all_results[best_feature_set][best_model_name]['model']
best_predictions = all_results[best_feature_set][best_model_name]['predictions']

# –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
X_best = df[feature_sets[best_feature_set]].values
X_train_best, X_test_best = X_best[train_indices], X_best[test_indices]
y_train_best, y_test_best = y_encoded[train_indices], y_encoded[test_indices]

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
imputer_best = SimpleImputer(strategy='median')
X_train_best_imputed = imputer_best.fit_transform(X_train_best)
X_test_best_imputed = imputer_best.transform(X_test_best)

scaler_best = StandardScaler()
X_train_best_scaled = scaler_best.fit_transform(X_train_best_imputed)
X_test_best_scaled = scaler_best.transform(X_test_best_imputed)

print("\n" + "="*100)
print(f"üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {best_model_name} ({best_feature_set})")
print("="*100)

# Classification report
print("\nüìä Classification Report:")
print(classification_report(y_test_best, best_predictions, target_names=le.classes_))

# –£–õ–£–ß–®–ï–ù–ò–ï: –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
if 'feature_importance' in all_results[best_feature_set][best_model_name]:
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {best_model_name}:")
    importance_data = all_results[best_feature_set][best_model_name]['feature_importance']
    importance_type = all_results[best_feature_set][best_model_name]['importance_type']
    
    print(f"–¢–∏–ø –≤–∞–∂–Ω–æ—Å—Ç–∏: {importance_type}")
    print("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, (feature, importance) in enumerate(importance_data[:10]):
        print(f"{i+1:2d}. {feature}: {importance:.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plt.figure(figsize=(12, 8))
    top_features = importance_data[:15]  # –¢–æ–ø-15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features, importances = zip(*top_features)
    
    plt.barh(range(len(features)), importances, color='skyblue', edgecolor='navy')
    plt.yticks(range(len(features)), features)
    plt.xlabel(importance_type)
    plt.title(f'–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - {best_model_name} ({best_feature_set})')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig('feature_importance_best_model.png', dpi=300, bbox_inches='tight')
    plt.show()

# Confusion Matrix
print("\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
viz_start_time = time.time()

plt.figure(figsize=(12, 10))
cm = confusion_matrix(y_test_best, best_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title(f'Confusion Matrix - {best_model_name} ({best_feature_set})')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('confusion_matrix_advanced.png', dpi=300, bbox_inches='tight')
plt.show()

# –ê–Ω–∞–ª–∏–∑ –ø–æ –∂–∞–Ω—Ä–∞–º
print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∂–∞–Ω—Ä–∞–º:")
genre_accuracy = {}
for i, genre in enumerate(le.classes_):
    genre_mask = y_test_best == i
    if np.sum(genre_mask) > 0:
        genre_acc = accuracy_score(y_test_best[genre_mask], best_predictions[genre_mask])
        genre_accuracy[genre] = genre_acc
        print(f"{genre}: {genre_acc:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∂–∞–Ω—Ä–∞–º
plt.figure(figsize=(12, 6))
genres = list(genre_accuracy.keys())
accuracies = list(genre_accuracy.values())

plt.bar(genres, accuracies, color='skyblue', edgecolor='navy')
plt.title(f'–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ –∂–∞–Ω—Ä–∞–º - {best_model_name} ({best_feature_set})')
plt.xlabel('–ñ–∞–Ω—Ä')
plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.grid(axis='y', alpha=0.3)

# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('genre_accuracy_advanced.png', dpi=300, bbox_inches='tight')
plt.show()

# –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")

# –°–æ–∑–¥–∞–µ–º heatmap –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
plt.figure(figsize=(16, 10))
pivot_df = comparison_df.pivot(index='Model', columns='Feature Set', values='Accuracy')
sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Accuracy'})
plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Advanced)')
plt.tight_layout()
plt.savefig('model_comparison_heatmap_advanced.png', dpi=300, bbox_inches='tight')
plt.show()

# –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
plt.figure(figsize=(14, 8))
best_models_data = []
for feature_set_name, feature_set_results in all_results.items():
    best_model_name = max(feature_set_results.keys(), 
                         key=lambda x: feature_set_results[x]['accuracy'])
    best_accuracy = feature_set_results[best_model_name]['accuracy']
    best_cv = feature_set_results[best_model_name]['cv_score']
    best_models_data.append({
        'Feature Set': feature_set_name,
        'Best Model': best_model_name,
        'Test Accuracy': best_accuracy,
        'CV Score': best_cv
    })

best_models_df = pd.DataFrame(best_models_data)
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
bars1 = ax1.bar(best_models_df['Feature Set'], best_models_df['Test Accuracy'], color=colors)
ax1.set_title('–¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π')
ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

for bar, acc, model in zip(bars1, best_models_df['Test Accuracy'], best_models_df['Best Model']):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{acc:.3f}\n({model})', ha='center', va='bottom', fontsize=9)

# CV Score
bars2 = ax2.bar(best_models_df['Feature Set'], best_models_df['CV Score'], color=colors)
ax2.set_title('CV Score –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π')
ax2.set_ylabel('CV Score')
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)

for bar, cv, model in zip(bars2, best_models_df['CV Score'], best_models_df['Best Model']):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{cv:.3f}\n({model})', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig('best_models_comparison_advanced.png', dpi=300, bbox_inches='tight')
plt.show()

viz_time = time.time() - viz_start_time
print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {format_time(viz_time)}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("\n" + "="*100)
print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*100)

save_start_time = time.time()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV
comparison_df.to_csv('classification_results_advanced.csv', index=False)
print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ classification_results_advanced.csv")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
import joblib
joblib.dump(best_model, 'best_genre_classifier_advanced.pkl')
print("‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ best_genre_classifier_advanced.pkl")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
best_model_info = {
    'feature_set': best_feature_set,
    'model_name': best_model_name,
    'test_accuracy': best_overall['Accuracy'],
    'cv_score': best_overall['CV Score'],
    'best_params': all_results[best_feature_set][best_model_name].get('params', {}),
    'feature_importance': all_results[best_feature_set][best_model_name].get('feature_importance', [])
}

import json
with open('best_model_info_advanced.json', 'w') as f:
    json.dump(best_model_info, f, indent=2, default=str)
print("‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ best_model_info_advanced.json")

# –£–õ–£–ß–®–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
importance_analysis = {}
for feature_set_name, feature_set_results in all_results.items():
    importance_analysis[feature_set_name] = {}
    for model_name, result in feature_set_results.items():
        if 'feature_importance' in result:
            importance_analysis[feature_set_name][model_name] = {
                'importance_type': result['importance_type'],
                'top_features': result['feature_importance'][:10]  # –¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            }

with open('feature_importance_analysis.json', 'w') as f:
    json.dump(importance_analysis, f, indent=2, default=str)
print("‚úÖ –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ feature_importance_analysis.json")

save_time = time.time() - save_start_time
print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {format_time(save_time)}")

# –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
total_time = time.time() - total_start_time
print("\n" + "="*100)
print("üéâ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï")
print("="*100)
print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {format_time(total_time)}")
print(f"üïê –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ: {datetime.now().strftime('%H:%M:%S')}")
print(f"üöÄ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
print(f"üìä –ù–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {best_feature_set}")
print(f"üéØ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {best_overall['Accuracy']:.4f}")
print(f"üìà CV Score: {best_overall['CV Score']:.4f}")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for feature_set_name, feature_set_results in all_results.items():
    best_acc = max(result['accuracy'] for result in feature_set_results.values())
    best_cv = max(result['cv_score'] for result in feature_set_results.values())
    print(f"- {feature_set_name}: Test {best_acc:.4f}, CV {best_cv:.4f}")

# –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
print(f"\n‚ùå –ù–∞–∏–±–æ–ª–µ–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∂–∞–Ω—Ä—ã:")
worst_genres = sorted(genre_accuracy.items(), key=lambda x: x[1])[:3]
for genre, acc in worst_genres:
    print(f"- {genre}: {acc:.4f}")

print(f"\n‚úÖ –ù–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –∂–∞–Ω—Ä—ã:")
best_genres = sorted(genre_accuracy.items(), key=lambda x: x[1], reverse=True)[:3]
for genre, acc in best_genres:
    print(f"- {genre}: {acc:.4f}")

# –£–õ–£–ß–®–ï–ù–ò–Ø: –°–≤–æ–¥–∫–∞ —É–ª—É—á—à–µ–Ω–∏–π
print(f"\nüöÄ –†–ï–ê–õ–ò–ó–û–í–ê–ù–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø:")
print("1. ‚úÖ –°–ª—É—á–∞–π–Ω–æ–µ stratified split –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ")
print("2. ‚úÖ –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è ensemble –º–æ–¥–µ–ª–µ–π")
print("3. ‚úÖ –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è tree-based –∏ –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
print("4. ‚úÖ Learning curves –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
print("5. ‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∂–∞–Ω—Ä–∞–º")
print("6. ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

print(f"\nüìà –£–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏:")
print("–ú–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ ensemble –º–µ—Ç–æ–¥–∞–º–∏ –ø–æ–∫–∞–∑–∞–ª–∏:")
print("- –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CV Score)")
print("- –õ—É—á—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å")
print("- –ë–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ –∂–∞–Ω—Ä–∞–º")
print("- –ö–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ª—É—á—à–∏—Ö ensemble")
print("- –ö–æ–Ω—Ç—Ä–æ–ª—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ learning curves")

print(f"\nüéä –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {format_time(total_time)}!") 